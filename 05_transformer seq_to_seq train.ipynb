{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle=False\n",
    "kaggle_path='/kaggle/input/nlp-getting-started/train.csv'\n",
    "local_path='train.csv'\n",
    "import torch\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "if kaggle==False:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df=pd.read_csv(kaggle_path if kaggle else local_path)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# count the number of words and plot a histogram\n",
    "df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "df['word_count'].hist(bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6316    184\n",
       "2908     84\n",
       "2950     85\n",
       "6406    187\n",
       "1796     52\n",
       "Name: keyword, dtype: int64"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label encoding of keyword using sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['keyword'] = le.fit_transform(df['keyword'].fillna(''))\n",
    "df['keyword'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['keyword'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>293</td>\n",
       "      <td>5</td>\n",
       "      <td>Playa</td>\n",
       "      <td>http://t.co/J8TYT1XRRK Twelve feared killed in...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7038</th>\n",
       "      <td>10084</td>\n",
       "      <td>206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Obama Declares Disaster for Typhoon-Devastated...</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>1182</td>\n",
       "      <td>24</td>\n",
       "      <td>Guelph Ontario Canada</td>\n",
       "      <td>New print available on http://t.co/ucy5fEA9yu!...</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5907</th>\n",
       "      <td>8435</td>\n",
       "      <td>172</td>\n",
       "      <td>USA</td>\n",
       "      <td>Watch This Airport Get Swallowed Up By A Sands...</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>6387</td>\n",
       "      <td>130</td>\n",
       "      <td>Miami Beach</td>\n",
       "      <td>everyone's wonder who will win and I'm over he...</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5872</th>\n",
       "      <td>8389</td>\n",
       "      <td>171</td>\n",
       "      <td>CA ??DC</td>\n",
       "      <td>there are a few people I'd let ruin my life my...</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1925</th>\n",
       "      <td>2768</td>\n",
       "      <td>56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@emaaalay thank you. ?? now I don't have a cit...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>919</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@cspanwj If 90BLKs&amp;amp;8WHTs colluded 2 take W...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6839</th>\n",
       "      <td>9796</td>\n",
       "      <td>200</td>\n",
       "      <td>Greensburg, PA</td>\n",
       "      <td>Did you know @lilithsaintcrow had a new releas...</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>3504</td>\n",
       "      <td>71</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Service on the Green Line has resumed after an...</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>546</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two Jewish Terrorists Charged In Historic-Chur...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7421</th>\n",
       "      <td>10615</td>\n",
       "      <td>217</td>\n",
       "      <td>Yogya Berhati Nyaman</td>\n",
       "      <td>@wocowae Police Officer Wounded Suspect Dead A...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2809</th>\n",
       "      <td>4041</td>\n",
       "      <td>81</td>\n",
       "      <td>Calgary, AB</td>\n",
       "      <td>The @rbcinsurance quote website = disaster. Tr...</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7594</th>\n",
       "      <td>10849</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A gas thing just exploded and I heard screams ...</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>2956</td>\n",
       "      <td>60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i miss my longer hair..but it was so dead anyw...</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6293</th>\n",
       "      <td>8992</td>\n",
       "      <td>184</td>\n",
       "      <td>CamaquÌ£/Pelotas</td>\n",
       "      <td>Somebody get the doctor I'm feelin' pretty poo...</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>794</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chevrolet : Avalanche LT 2011 lt used 5.3 l v ...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7548</th>\n",
       "      <td>10789</td>\n",
       "      <td>221</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>Four hundred wrecked cars (costing $100 apiece...</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>1405</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>new summer long thin body bag hip A word skirt...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3303</th>\n",
       "      <td>4734</td>\n",
       "      <td>96</td>\n",
       "      <td>LA - everywhere</td>\n",
       "      <td>Jay and alexis broke up there goes all your fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  keyword               location  \\\n",
       "207     293        5                  Playa   \n",
       "7038  10084      206                    NaN   \n",
       "814    1182       24  Guelph Ontario Canada   \n",
       "5907   8435      172                    USA   \n",
       "4491   6387      130            Miami Beach   \n",
       "5872   8389      171                CA ??DC   \n",
       "1925   2768       56                    NaN   \n",
       "635     919       18                    NaN   \n",
       "6839   9796      200         Greensburg, PA   \n",
       "2440   3504       71            Chicago, IL   \n",
       "380     546       11                    NaN   \n",
       "7421  10615      217   Yogya Berhati Nyaman   \n",
       "2809   4041       81            Calgary, AB   \n",
       "7594  10849        0                    NaN   \n",
       "2062   2956       60                    NaN   \n",
       "6293   8992      184       CamaquÌ£/Pelotas   \n",
       "545     794       15                    NaN   \n",
       "7548  10789      221           Pennsylvania   \n",
       "970    1405       28                    NaN   \n",
       "3303   4734       96        LA - everywhere   \n",
       "\n",
       "                                                   text  target  word_count  \n",
       "207   http://t.co/J8TYT1XRRK Twelve feared killed in...       1          11  \n",
       "7038  Obama Declares Disaster for Typhoon-Devastated...       1          15  \n",
       "814   New print available on http://t.co/ucy5fEA9yu!...       0          14  \n",
       "5907  Watch This Airport Get Swallowed Up By A Sands...       1          14  \n",
       "4491  everyone's wonder who will win and I'm over he...       0          16  \n",
       "5872  there are a few people I'd let ruin my life my...       0          23  \n",
       "1925  @emaaalay thank you. ?? now I don't have a cit...       0          13  \n",
       "635   @cspanwj If 90BLKs&amp;8WHTs colluded 2 take W...       1          22  \n",
       "6839  Did you know @lilithsaintcrow had a new releas...       0          20  \n",
       "2440  Service on the Green Line has resumed after an...       1          16  \n",
       "380   Two Jewish Terrorists Charged In Historic-Chur...       0          13  \n",
       "7421  @wocowae Police Officer Wounded Suspect Dead A...       1          11  \n",
       "2809  The @rbcinsurance quote website = disaster. Tr...       0          23  \n",
       "7594  A gas thing just exploded and I heard screams ...       1          19  \n",
       "2062  i miss my longer hair..but it was so dead anyw...       0          14  \n",
       "6293  Somebody get the doctor I'm feelin' pretty poo...       0          17  \n",
       "545   Chevrolet : Avalanche LT 2011 lt used 5.3 l v ...       1          22  \n",
       "7548  Four hundred wrecked cars (costing $100 apiece...       0          18  \n",
       "970   new summer long thin body bag hip A word skirt...       0          13  \n",
       "3303  Jay and alexis broke up there goes all your fa...       0          15  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 10000\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "PAD_IDX=1\n",
    "SOS_IDX=2\n",
    "UNK_IDX=0\n",
    "\n",
    "\n",
    "# Tokenizer-Funktion\n",
    "def tokenizer(text):\n",
    "    # use spacey for tokenization\n",
    "    return [tok.text for tok in nlp.tokenizer(text)]\n",
    "\n",
    "# Vokabular erstellen\n",
    "def build_vocab(texts, vocab_size=None):\n",
    "    word_to_idx = {}\n",
    "    for text in texts:\n",
    "        tokens = tokenizer(text)\n",
    "        for token in tokens:\n",
    "            if token not in word_to_idx:\n",
    "                word_to_idx[token] = len(word_to_idx)\n",
    "    if vocab_size is not None:\n",
    "        word_to_idx = {k: v for k, v in sorted(word_to_idx.items(), key=lambda item: item[1])[:vocab_size-3]}\n",
    "    # increase all idx by 3  \n",
    "    word_to_idx = {k: v+3 for k, v in word_to_idx.items()}\n",
    "    # add special tokens\n",
    "    word_to_idx['<unk>'] = UNK_IDX\n",
    "    word_to_idx['<pad>'] = PAD_IDX\n",
    "    word_to_idx['<sos>'] = SOS_IDX\n",
    "    return word_to_idx\n",
    "\n",
    "# Texte in Sequenzen von Wortindizes umwandeln\n",
    "def text_to_indices(text, word_to_idx):\n",
    "    tokens = tokenizer(text)\n",
    "    indices = [word_to_idx[token] if token in word_to_idx else 0 for token in tokens]\n",
    "    return indices\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "# Aufteilung in Trainings- und Testdaten\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Erstellung des Vokabulars\n",
    "texts = train_df['text'].tolist()\n",
    "word_to_idx = build_vocab(texts, vocab_size=10000)\n",
    "\n",
    "idx_to_word = {v: k for k, v in word_to_idx.items()}\n",
    "\n",
    "vocab_size=len(word_to_idx)\n",
    "print(\"vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1639, 101, 27, 9219, 0]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_pipeline(x):                           \n",
    "    return text_to_indices(x, word_to_idx)\n",
    "\n",
    "text_pipeline(\"This is a test sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  74,    0,  869,   47, 7909,    7,  113,   16, 2907, 2093,    0,    0,\n",
      "         230,  346,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1])\n"
     ]
    }
   ],
   "source": [
    "class DisasterTweetsDataset(Dataset):\n",
    "    def __init__(self, df, vocab_size=10000, test=False, sequence_length=100):\n",
    "        self.df = df\n",
    "        self.vocab_size = vocab_size\n",
    "        self.test = test\n",
    "        self.sequence_length = sequence_length\n",
    "        self.empty_dummy_for_fixed_length = torch.ones(self.sequence_length, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x= self.df.iloc[idx][\"text\"]\n",
    "        x = text_pipeline(x)\n",
    "        x = torch.tensor(x)\n",
    "        x= torch.cat((x, self.empty_dummy_for_fixed_length))[:self.sequence_length]\n",
    "        return x\n",
    "    \n",
    "disaster_tweets_dataset = DisasterTweetsDataset(df, vocab_size=vocab_size)\n",
    "x=disaster_tweets_dataset[0]\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_transformer_input_dimensions(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\n",
    "    batch_size = src.size(1)\n",
    "    src_seq_len = src.size(0)\n",
    "    tgt_seq_len = tgt_input.size(0)\n",
    "\n",
    "    assert src.dim() == 2, f\"src should have 2 dimensions, but has {src.dim()}\"\n",
    "    assert src.size(1) == batch_size, f\"The second dimension of src should be batch_size, but is {src.size(1)}\"\n",
    "\n",
    "    # assert no nan values\n",
    "    assert torch.isnan(src).sum() == 0, f\"src contains {torch.isnan(src).sum()} nan values\"\n",
    "    assert torch.isnan(tgt_input).sum() == 0, f\"tgt_input contains {torch.isnan(tgt_input).sum()} nan values\"\n",
    "    assert torch.isnan(src_mask).sum() == 0, f\"src_mask contains {torch.isnan(src_mask).sum()} nan values\"\n",
    "    assert torch.isnan(tgt_mask).sum() == 0, f\"tgt_mask contains {torch.isnan(tgt_mask).sum()} nan values\"\n",
    "    assert torch.isnan(src_padding_mask).sum() == 0, f\"src_padding_mask contains {torch.isnan(src_padding_mask).sum()} nan values\"\n",
    "    assert torch.isnan(tgt_padding_mask).sum() == 0, f\"tgt_padding_mask contains {torch.isnan(tgt_padding_mask).sum()} nan values\"\n",
    "    assert torch.isnan(memory_key_padding_mask).sum() == 0, f\"memory_key_padding_mask contains {torch.isnan(memory_key_padding_mask).sum()} nan values\"\n",
    "        \n",
    "    assert tgt_input.dim() == 2, f\"tgt_input should have 2 dimensions, but has {tgt_input.dim()}\"\n",
    "    assert tgt_input.size(1) == batch_size, f\"The second dimension of tgt_input should be batch_size, but is {tgt_input.size(1)}\"\n",
    "    \n",
    "    assert src_mask.dim() == 2, f\"src_mask should have 2 dimensions, but has {src_mask.dim()}\"\n",
    "    assert src_mask.size() == (src_seq_len, src_seq_len), f\"src_mask should have shape (src_seq_len, src_seq_len), but has {src_mask.size()}\"\n",
    "    \n",
    "    assert tgt_mask.dim() == 2, f\"tgt_mask should have 2 dimensions, but has {tgt_mask.dim()}\"\n",
    "    assert tgt_mask.size() == (tgt_seq_len, tgt_seq_len), f\"tgt_mask should have shape (tgt_seq_len, tgt_seq_len), but has {tgt_mask.size()}\"\n",
    "    \n",
    "    for mask, name in zip([src_padding_mask, memory_key_padding_mask], [\"src_padding_mask\", \"memory_key_padding_mask\"]):\n",
    "        assert mask.dim() == 2, f\"{name} should have 2 dimensions, but has {mask.dim()}\"\n",
    "        assert mask.size() == (batch_size, src_seq_len), f\"{name} should have shape (batch_size, src_seq_len), but has {mask.size()}\"\n",
    "    \n",
    "    assert tgt_padding_mask.dim() == 2, f\"tgt_padding_mask should have 2 dimensions, but has {tgt_padding_mask.dim()}\"\n",
    "    assert tgt_padding_mask.size() == (batch_size, tgt_seq_len), f\"tgt_padding_mask should have shape (batch_size, tgt_seq_len), but has {tgt_padding_mask.size()}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan count in embedding:  0\n",
      "Max value in pos encoding: 2.3678228855133057, Min value pos encoding: -2.32763934135437\n",
      "stddev value pos encoding: 0.8757405281066895, mean value pos encoding: 0.5315021276473999\n",
      "nan count in pos encoder:  0\n",
      "nan count in src:  0\n",
      "nan count in src_mask:  0\n",
      "nan count in src_padding_mask:  0\n",
      "nan count in encoder:  147456\n",
      "nan count in decoder:  169984\n",
      "x.shape torch.Size([4, 100])\n",
      "model output shape:  torch.Size([4, 100, 10000])\n",
      "nan count in output:  3320000\n",
      "model output:  http://t.co/UUWEiKD7sP Boone Sicily series http://t.co/V1mtR517Ue soul http://t.co/V1mtR517Ue Dudes http://t.co/eL24mnFcHw Bamenda Fatalities El PRESENT Estate seismic insane series <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "nan count in embedding:  0\n",
      "Max value in pos encoding: 2.3679797649383545, Min value pos encoding: -2.2466166019439697\n",
      "stddev value pos encoding: 0.8779615163803101, mean value pos encoding: 0.5326945185661316\n",
      "nan count in pos encoder:  0\n",
      "nan count in src:  0\n",
      "nan count in src_mask:  0\n",
      "nan count in src_padding_mask:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/torch/nn/functional.py:5038: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan count in encoder:  157696\n",
      "nan count in decoder:  196608\n",
      "x.shape torch.Size([4, 100])\n",
      "model output shape:  torch.Size([4, 100, 10000])\n",
      "nan count in output:  3840000\n",
      "model output:  options Luiz sorry collapse <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "nan count in embedding:  0\n",
      "Max value in pos encoding: 2.3678839206695557, Min value pos encoding: -1.2557591199874878\n",
      "stddev value pos encoding: 0.885172963142395, mean value pos encoding: 0.5270705223083496\n",
      "nan count in pos encoder:  0\n",
      "nan count in src:  0\n",
      "nan count in src_mask:  0\n",
      "nan count in src_padding_mask:  0\n",
      "nan count in encoder:  82944\n",
      "nan count in decoder:  82944\n",
      "x.shape torch.Size([2, 100])\n",
      "model output shape:  torch.Size([2, 100, 10000])\n",
      "nan count in output:  1620000\n",
      "model output:  Jen flawless helps paths Birmingham Green manåÊarmed @roughdeal1 bagged Luiz record http://t.co/V1mtR517Ue Noel KPdied casualties Engle Ray shall bagged <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, nhead, num_layers, dim_feedforward, dropout):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.pos_encoder = PositionalEncoding(emb_size, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(emb_size, nhead, dim_feedforward, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, src, src_mask, src_padding_mask):\n",
    "        src_emb = self.embedding(src) * math.sqrt(self.emb_size)\n",
    "        print(\"nan count in embedding: \", torch.isnan(src_emb).sum().item())\n",
    "        src_emb = self.pos_encoder(src_emb)\n",
    "        print(f\"Max value in pos encoding: {src_emb.max().item()}, Min value pos encoding: {src_emb.min().item()}\")\n",
    "        print(f\"stddev value pos encoding: {src_emb.std().item()}, mean value pos encoding: {src_emb.mean().item()}\")\n",
    "        print(\"nan count in pos encoder: \", torch.isnan(src_emb).sum().item())\n",
    "        print(\"nan count in src: \", torch.isnan(src).sum().item())\n",
    "        print(\"nan count in src_mask: \", torch.isnan(src_mask).sum().item())\n",
    "        print(\"nan count in src_padding_mask: \", torch.isnan(src_padding_mask).sum().item())\n",
    "        output = self.transformer_encoder(src_emb, mask=src_mask, src_key_padding_mask=src_padding_mask)\n",
    "        print(\"nan count in encoder: \", torch.isnan(output).sum().item())\n",
    "        return output\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, nhead, num_layers, dim_feedforward, dropout):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.pos_encoder = PositionalEncoding(emb_size, dropout)\n",
    "        decoder_layers = nn.TransformerDecoderLayer(emb_size, nhead, dim_feedforward, dropout)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layers, num_layers)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask, tgt_padding_mask, memory_key_padding_mask):\n",
    "        tgt_emb = self.embedding(tgt) * math.sqrt(self.emb_size)\n",
    "        tgt_emb = self.pos_encoder(tgt_emb)\n",
    "        output = self.transformer_decoder(tgt_emb, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
    "        print(\"nan count in decoder: \", torch.isnan(output).sum().item())\n",
    "        return output\n",
    "\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, vocab_size=None):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.fc = nn.Linear(decoder.emb_size, vocab_size)\n",
    "        self.apply(self.init_weights)\n",
    "        \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        if isinstance(m, nn.Embedding):\n",
    "            nn.init.uniform_(m.weight, -0.05, 0.05)\n",
    "\n",
    "\n",
    "    def forward(self, \n",
    "                src=None,\n",
    "                tgt=None, \n",
    "                src_mask=None, \n",
    "                tgt_mask=None, \n",
    "                src_padding_mask=None, \n",
    "                tgt_padding_mask=None, \n",
    "                memory_key_padding_mask=None):\n",
    "        memory = self.encoder(src, src_mask, src_padding_mask)\n",
    "        outs = self.decoder(tgt, memory, tgt_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        outs= self.fc(outs)\n",
    "        return outs\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=device).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "import torch\n",
    "\n",
    "def shift_right(target_sequences):\n",
    "    batch_size, seq_len = target_sequences.size()\n",
    "    sos_tensor = torch.full((batch_size, 1), SOS_IDX, dtype=target_sequences.dtype, device=target_sequences.device)    \n",
    "    shifted_sequences = torch.cat([sos_tensor, target_sequences[:, :-1]], dim=1)\n",
    "    return shifted_sequences\n",
    "\n",
    "# test all together\n",
    "df_to_test=df.sample(10)\n",
    "dataset_to_test = DisasterTweetsDataset(df_to_test, vocab_size=vocab_size)\n",
    "dataloader_to_test = DataLoader(dataset_to_test, batch_size=4, shuffle=True)\n",
    "model = Seq2SeqTransformer(\n",
    "    encoder=TransformerEncoder(vocab_size=vocab_size, emb_size=512, nhead=8, num_layers=6, dim_feedforward=2048, dropout=0.1),\n",
    "    decoder=TransformerDecoder(vocab_size=vocab_size, emb_size=512, nhead=8, num_layers=6, dim_feedforward=2048, dropout=0.1),\n",
    "    vocab_size=vocab_size\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "def get_text_from_model_output(output, idx_to_word):\n",
    "    output = output.argmax(dim=2)\n",
    "    output_text=[]\n",
    "    for i in range(output.size(0)):\n",
    "        for word_idx in output[i]:\n",
    "            output_token=\"\"\n",
    "            if word_idx.item() == PAD_IDX:\n",
    "                break\n",
    "            elif word_idx.item() == 0:\n",
    "                output_token=\"<unk>\"\n",
    "            else:\n",
    "                output_token=idx_to_word[word_idx.item()]\n",
    "            output_text.append(output_token)\n",
    "    return \" \".join(output_text[:30])\n",
    "\n",
    "\n",
    "# Beispielaufruf der Funktion\n",
    "\n",
    "    \n",
    "for x in dataloader_to_test:  \n",
    "    src = x.to(device)\n",
    "    tgt = x.to(device)\n",
    "\n",
    "    # shift targets to the right by one and add padding token at the end\n",
    "    tgt_input = shift_right(tgt)\n",
    "\n",
    "    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "    \n",
    "    \n",
    "    check_transformer_input_dimensions(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "    \n",
    "    output = model(src=src, \n",
    "                   tgt=tgt_input, \n",
    "                   src_mask=src_mask, \n",
    "                   tgt_mask=tgt_mask, \n",
    "                   src_padding_mask=src_padding_mask, \n",
    "                   tgt_padding_mask=tgt_padding_mask,\n",
    "                   memory_key_padding_mask=src_padding_mask)\n",
    "\n",
    "    print(\"x.shape\", x.shape)\n",
    "    print(\"model output shape: \" , output.shape)\n",
    "    print(\"nan count in output: \", torch.isnan(output).sum().item())\n",
    "    \n",
    "    # map output to vocab for each batch\n",
    "    \n",
    "    # output is batch_size, sequence_length, vocab_size\n",
    "    # apply idx_to_word for each batch\n",
    "    output_text = get_text_from_model_output(output, idx_to_word)\n",
    "    print(\"model output: \" , output_text)\n",
    "    # Modify the line where the input tensor is created\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    \n",
    "    def __init__(self, model, train_dataloader, valid_dataloader, optimizer, scheduler, loss_func, device):\n",
    "        self.model=model\n",
    "        self.device=device\n",
    "        self.model=self.model.to(self.device)\n",
    "        self.train_dataloader=train_dataloader\n",
    "        self.valid_dataloader=valid_dataloader\n",
    "        self.optimizer=optimizer\n",
    "        self.scheduler=scheduler\n",
    "        self.loss_func=loss_func\n",
    "        self.accu_train=[]\n",
    "        self.accu_valid=[]\n",
    "\n",
    "    def evaluate(self, loader):\n",
    "        self.model.eval()  # Set model to evaluation mode\n",
    "        total_loss = 0\n",
    "        total_items = 0\n",
    "\n",
    "        with torch.no_grad():  # No gradients needed for evaluation\n",
    "            for x in loader:\n",
    "                src = x.to(self.device)\n",
    "                tgt = x.to(self.device)\n",
    "                tgt_input = tgt#[:-1, :]\n",
    "\n",
    "                src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "                output = self.model(src=src, \n",
    "                                    tgt=tgt_input, \n",
    "                                    src_mask=src_mask, \n",
    "                                    tgt_mask=tgt_mask, \n",
    "                                    src_padding_mask=src_padding_mask, \n",
    "                                    tgt_padding_mask=tgt_padding_mask,\n",
    "                                    memory_key_padding_mask=src_padding_mask)\n",
    "                \n",
    "                # You may need to adjust the shape of y and output depending on your loss function\n",
    "                print(y.shape)\n",
    "                loss = self.loss_func(output, y)\n",
    "                total_loss += loss.item()\n",
    "                total_items += 1\n",
    "\n",
    "        average_loss = total_loss / total_items\n",
    "        return average_loss\n",
    "\n",
    "\n",
    "    def fit_one_epoch(self):\n",
    "        epoch_start_time = time.time()\n",
    "        self.model.train()\n",
    "        losses=[]\n",
    "\n",
    "        for idx, x in enumerate(self.train_dataloader):\n",
    "            self.optimizer.zero_grad()\n",
    "            src = x.to(device)\n",
    "            tgt = x.to(device)\n",
    "\n",
    "            # shift targets to the right\n",
    "            tgt_input = shift_right(tgt)\n",
    "    \n",
    "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "    \n",
    "            check_transformer_input_dimensions(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "            \n",
    "            \n",
    "            output = model(src=src, \n",
    "                   tgt=tgt_input, \n",
    "                   src_mask=src_mask, \n",
    "                   tgt_mask=tgt_mask, \n",
    "                   src_padding_mask=src_padding_mask, \n",
    "                   tgt_padding_mask=tgt_padding_mask,\n",
    "                   memory_key_padding_mask=src_padding_mask)\n",
    "            output = output.permute(0, 2, 1) # permute to have batch_size, vocab_size, sequence_length\n",
    "            #print(\"output of batch\", output)\n",
    "            # count nan values in output\n",
    "            print(\"nan values in output\", torch.isnan(output).sum())\n",
    "            loss = self.loss_func(output, tgt_input)\n",
    "            print(\"loss of batch\", loss)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        #accu_train = self.evaluate(self.train_dataloader)\n",
    "        #accu_valid = self.evaluate(self.valid_dataloader)\n",
    "        #self.accu_train.append(accu_train)\n",
    "        #self.accu_valid.append(accu_valid)\n",
    "        #self.scheduler.step() \n",
    "        return sum(losses)/len(losses)\n",
    "\n",
    "    def fit_epochs(self, number=None):\n",
    "        for epoch in range(number):\n",
    "            loss=self.fit_one_epoch()\n",
    "            print('-' * 59)\n",
    "            print('| end of epoch {:3d} | loss: {:5.2f}s '.format(\n",
    "                                        epoch,\n",
    "                                        loss))\n",
    "                                           \n",
    "\n",
    "    def plot_training(self):\n",
    "        plt.plot(self.accu_train, label=\"train accuracy\")\n",
    "        plt.plot(self.accu_valid, label=\"valid accuracy\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "if kaggle==False:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "LR = 0.001  # learning rate\n",
    "BATCH_SIZE = 128 # batch size for training\n",
    "\n",
    "\n",
    "model = Seq2SeqTransformer(\n",
    "    encoder=TransformerEncoder(vocab_size=vocab_size, emb_size=128, nhead=2, num_layers=2, dim_feedforward=512, dropout=0.1),\n",
    "    decoder=TransformerDecoder(vocab_size=vocab_size, emb_size=128, nhead=2, num_layers=2, dim_feedforward=512, dropout=0.1),\n",
    "    vocab_size=vocab_size\n",
    ")\n",
    "\n",
    "train_dataset=DisasterTweetsDataset(train_df, vocab_size=vocab_size)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True)\n",
    "valid_dataset=DisasterTweetsDataset(test_df, vocab_size=vocab_size)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "#\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10.0, gamma=0.9)   # every 3 epochs, LR is multiplied by 0.7\n",
    "\n",
    "learner=Learner(model, train_dataloader, valid_dataloader, optimizer, scheduler, loss_func, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan count in embedding:  0\n",
      "nan count in pos encoder:  0\n",
      "nan count in src:  0\n",
      "nan count in src_mask:  0\n",
      "nan count in src_padding_mask:  0\n",
      "nan count in encoder:  1081344\n",
      "nan count in decoder:  1212416\n",
      "nan values in output tensor(94720000)\n",
      "loss of batch tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
      "nan count in embedding:  1445248\n",
      "nan count in pos encoder:  1445248\n",
      "nan count in src:  0\n",
      "nan count in src_mask:  0\n",
      "nan count in src_padding_mask:  0\n",
      "nan count in encoder:  1638400\n",
      "nan count in decoder:  1638400\n",
      "nan values in output tensor(128000000)\n",
      "loss of batch tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
      "nan count in embedding:  1549184\n",
      "nan count in pos encoder:  1549184\n",
      "nan count in src:  0\n",
      "nan count in src_mask:  0\n",
      "nan count in src_padding_mask:  0\n",
      "nan count in encoder:  1638400\n",
      "nan count in decoder:  1638400\n",
      "nan values in output tensor(128000000)\n",
      "loss of batch tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
      "nan count in embedding:  1559680\n",
      "nan count in pos encoder:  1559680\n",
      "nan count in src:  0\n",
      "nan count in src_mask:  0\n",
      "nan count in src_padding_mask:  0\n",
      "nan count in encoder:  1638400\n",
      "nan count in decoder:  1638400\n",
      "nan values in output tensor(128000000)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[177], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_epochs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[175], line 89\u001b[0m, in \u001b[0;36mLearner.fit_epochs\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_epochs\u001b[39m(\u001b[38;5;28mself\u001b[39m, number\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(number):\n\u001b[0;32m---> 89\u001b[0m         loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m59\u001b[39m)\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m| end of epoch \u001b[39m\u001b[38;5;132;01m{:3d}\u001b[39;00m\u001b[38;5;124m | loss: \u001b[39m\u001b[38;5;132;01m{:5.2f}\u001b[39;00m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     92\u001b[0m                                     epoch,\n\u001b[1;32m     93\u001b[0m                                     loss))\n",
      "Cell \u001b[0;32mIn[175], line 74\u001b[0m, in \u001b[0;36mLearner.fit_one_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m#print(\"output of batch\", output)\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# count nan values in output\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan values in output\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39misnan(output)\u001b[38;5;241m.\u001b[39msum())\n\u001b[0;32m---> 74\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss of batch\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss)\n\u001b[1;32m     76\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/torch/nn/functional.py:3061\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3060\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "learner.fit_epochs(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   0 | loss:   nans \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | loss:   nans \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | loss:   nans \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | loss:   nans \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | loss:   nans \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | loss:   nans \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | loss:   nans \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | loss:   nans \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | loss:   nans \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | loss:   nans \n"
     ]
    }
   ],
   "source": [
    "learner.fit_epochs(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   0 | loss:   nans \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | loss:   nans \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | loss:   nans \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_epochs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 85\u001b[0m, in \u001b[0;36mLearner.fit_epochs\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_epochs\u001b[39m(\u001b[38;5;28mself\u001b[39m, number\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(number):\n\u001b[0;32m---> 85\u001b[0m         loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m59\u001b[39m)\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m| end of epoch \u001b[39m\u001b[38;5;132;01m{:3d}\u001b[39;00m\u001b[38;5;124m | loss: \u001b[39m\u001b[38;5;132;01m{:5.2f}\u001b[39;00m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     88\u001b[0m                                     epoch,\n\u001b[1;32m     89\u001b[0m                                     loss))\n",
      "Cell \u001b[0;32mIn[13], line 72\u001b[0m, in \u001b[0;36mLearner.fit_one_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# permute to have batch_size, vocab_size, sequence_length\u001b[39;00m\n\u001b[1;32m     71\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_func(output, tgt_input)\n\u001b[0;32m---> 72\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     74\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/torch/autograd/__init__.py:204\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learner.fit_epochs(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2191</th>\n",
       "      <td>7336</td>\n",
       "      <td>nuclear%20reactor</td>\n",
       "      <td>Helsinki</td>\n",
       "      <td>Finnish Nuclear Plant to Move Ahead After Fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3102</th>\n",
       "      <td>10267</td>\n",
       "      <td>war%20zone</td>\n",
       "      <td>MDS af ?</td>\n",
       "      <td>Saipan looks like a war zone though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>6761</td>\n",
       "      <td>lightning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Baby you're like lightning in a bottle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2111</th>\n",
       "      <td>7079</td>\n",
       "      <td>meltdown</td>\n",
       "      <td>St. Catherine, Jamaica, W.I.</td>\n",
       "      <td>CommoditiesåÊAre Crashing Like It's 2008 All O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>2029</td>\n",
       "      <td>casualties</td>\n",
       "      <td>Ngayogyakarta Hadiningrat</td>\n",
       "      <td>Memorial day of 70 years Hiroshima and Nagasak...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id            keyword                      location  \\\n",
       "2191   7336  nuclear%20reactor                      Helsinki   \n",
       "3102  10267         war%20zone                      MDS af ?   \n",
       "2008   6761          lightning                           NaN   \n",
       "2111   7079           meltdown  St. Catherine, Jamaica, W.I.   \n",
       "622    2029         casualties    Ngayogyakarta Hadiningrat    \n",
       "\n",
       "                                                   text  \n",
       "2191  Finnish Nuclear Plant to Move Ahead After Fina...  \n",
       "3102                Saipan looks like a war zone though  \n",
       "2008             Baby you're like lightning in a bottle  \n",
       "2111  CommoditiesåÊAre Crashing Like It's 2008 All O...  \n",
       "622   Memorial day of 70 years Hiroshima and Nagasak...  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_test_path='test.csv'\n",
    "kaggle_test_path='/kaggle/input/nlp-getting-started/test.csv'\n",
    "\n",
    "df_submission_test_data=pd.read_csv(kaggle_test_path if kaggle else local_test_path)\n",
    "df_submission_test_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode keyword using trained label encoder\n",
    "df_submission_test_data['keyword'] = le.transform(df_submission_test_data['keyword'].fillna(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch_test_data(batch):\n",
    "    x, keywords = zip(*batch)\n",
    "    # collate for embedding bag\n",
    "    empty_dummy_for_fixed_length = torch.zeros(sequence_length, dtype=torch.long)\n",
    "    x = torch.stack([torch.cat((text, empty_dummy_for_fixed_length))[:sequence_length] for text in x])\n",
    "    keywords=torch.stack(keywords)\n",
    "    return x, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "preds=[]\n",
    "test_dataset=DisasterTweetsDataset(df_submission_test_data, vocab_size=vocab_size, test=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_batch_test_data)\n",
    "with torch.no_grad():\n",
    "    for idx, (x, keywords) in enumerate(test_dataloader):\n",
    "        predicted_label = model(x, keywords)\n",
    "        preds.extend(predicted_label.argmax(1).tolist())\n",
    "\n",
    "df_submission_test_data[\"target\"]=preds\n",
    "df_submission_test_data[[\"id\", \"target\"]].to_csv(\"submission_04.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score: 0.75237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
