{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle=False\n",
    "kaggle_path='/kaggle/input/nlp-getting-started/train.csv'\n",
    "local_path='train.csv'\n",
    "import os\n",
    "if kaggle:\n",
    "    os.system('pip install neptune')\n",
    "    \n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "# models: https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "\n",
    "\n",
    "df=pd.read_csv(kaggle_path if kaggle else local_path)\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fatalities               45\n",
       "deluge                   42\n",
       "armageddon               42\n",
       "sinking                  41\n",
       "damage                   41\n",
       "                         ..\n",
       "forest%20fire            19\n",
       "epicentre                12\n",
       "threat                   11\n",
       "inundation               10\n",
       "radiation%20emergency     9\n",
       "Name: keyword, Length: 221, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keyword.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'ablaze', 'accident', 'aftershock', 'airplane%20accident',\n",
       "       'ambulance', 'annihilated', 'annihilation', 'apocalypse',\n",
       "       'armageddon', 'army', 'arson', 'arsonist', 'attack', 'attacked',\n",
       "       'avalanche', 'battle', 'bioterror', 'bioterrorism', 'blaze',\n",
       "       'blazing', 'bleeding', 'blew%20up', 'blight', 'blizzard', 'blood',\n",
       "       'bloody', 'blown%20up', 'body%20bag', 'body%20bagging',\n",
       "       'body%20bags', 'bomb', 'bombed', 'bombing', 'bridge%20collapse',\n",
       "       'buildings%20burning', 'buildings%20on%20fire', 'burned',\n",
       "       'burning', 'burning%20buildings', 'bush%20fires', 'casualties',\n",
       "       'casualty', 'catastrophe', 'catastrophic', 'chemical%20emergency',\n",
       "       'cliff%20fall', 'collapse', 'collapsed', 'collide', 'collided',\n",
       "       'collision', 'crash', 'crashed', 'crush', 'crushed', 'curfew',\n",
       "       'cyclone', 'damage', 'danger', 'dead', 'death', 'deaths', 'debris',\n",
       "       'deluge', 'deluged', 'demolish', 'demolished', 'demolition',\n",
       "       'derail', 'derailed', 'derailment', 'desolate', 'desolation',\n",
       "       'destroy', 'destroyed', 'destruction', 'detonate', 'detonation',\n",
       "       'devastated', 'devastation', 'disaster', 'displaced', 'drought',\n",
       "       'drown', 'drowned', 'drowning', 'dust%20storm', 'earthquake',\n",
       "       'electrocute', 'electrocuted', 'emergency', 'emergency%20plan',\n",
       "       'emergency%20services', 'engulfed', 'epicentre', 'evacuate',\n",
       "       'evacuated', 'evacuation', 'explode', 'exploded', 'explosion',\n",
       "       'eyewitness', 'famine', 'fatal', 'fatalities', 'fatality', 'fear',\n",
       "       'fire', 'fire%20truck', 'first%20responders', 'flames',\n",
       "       'flattened', 'flood', 'flooding', 'floods', 'forest%20fire',\n",
       "       'forest%20fires', 'hail', 'hailstorm', 'harm', 'hazard',\n",
       "       'hazardous', 'heat%20wave', 'hellfire', 'hijack', 'hijacker',\n",
       "       'hijacking', 'hostage', 'hostages', 'hurricane', 'injured',\n",
       "       'injuries', 'injury', 'inundated', 'inundation', 'landslide',\n",
       "       'lava', 'lightning', 'loud%20bang', 'mass%20murder',\n",
       "       'mass%20murderer', 'massacre', 'mayhem', 'meltdown', 'military',\n",
       "       'mudslide', 'natural%20disaster', 'nuclear%20disaster',\n",
       "       'nuclear%20reactor', 'obliterate', 'obliterated', 'obliteration',\n",
       "       'oil%20spill', 'outbreak', 'pandemonium', 'panic', 'panicking',\n",
       "       'police', 'quarantine', 'quarantined', 'radiation%20emergency',\n",
       "       'rainstorm', 'razed', 'refugees', 'rescue', 'rescued', 'rescuers',\n",
       "       'riot', 'rioting', 'rubble', 'ruin', 'sandstorm', 'screamed',\n",
       "       'screaming', 'screams', 'seismic', 'sinkhole', 'sinking', 'siren',\n",
       "       'sirens', 'smoke', 'snowstorm', 'storm', 'stretcher',\n",
       "       'structural%20failure', 'suicide%20bomb', 'suicide%20bomber',\n",
       "       'suicide%20bombing', 'sunk', 'survive', 'survived', 'survivors',\n",
       "       'terrorism', 'terrorist', 'threat', 'thunder', 'thunderstorm',\n",
       "       'tornado', 'tragedy', 'trapped', 'trauma', 'traumatised',\n",
       "       'trouble', 'tsunami', 'twister', 'typhoon', 'upheaval',\n",
       "       'violent%20storm', 'volcano', 'war%20zone', 'weapon', 'weapons',\n",
       "       'whirlwind', 'wild%20fires', 'wildfire', 'windstorm', 'wounded',\n",
       "       'wounds', 'wreck', 'wreckage', 'wrecked'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keyword.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# str replaye %20 to space\n",
    "\n",
    "def prepare_df(df):\n",
    "    df=df.copy()\n",
    "    df.keyword=df.keyword.str.replace('%20',' ')\n",
    "    df.keyword=df.keyword.fillna('none')\n",
    "    df['text']=df['keyword'] + ' ' + df['text']\n",
    "    return df\n",
    "\n",
    "df=prepare_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset and dataloader\n",
    "\n",
    "SEQ_LEN = 30\n",
    "KEYWORD_MAX_SEQ_LEN=3\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, test=False):\n",
    "        self.df = df\n",
    "        self.test = test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text=self.df.iloc[idx]['text']\n",
    "\n",
    "        input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        if (len(input_ids)<SEQ_LEN):\n",
    "            input_ids=input_ids+[0]*(SEQ_LEN-len(input_ids))\n",
    "            attention_mask=attention_mask+[0]*(SEQ_LEN-len(attention_mask))\n",
    "        elif (len(input_ids)>SEQ_LEN):\n",
    "            input_ids=input_ids[:SEQ_LEN]\n",
    "            attention_mask=attention_mask[:SEQ_LEN]\n",
    "\n",
    "        \n",
    "        keyword=self.df.iloc[idx]['keyword']\n",
    "        keyword_ids = tokenizer.encode(keyword, add_special_tokens=True)\n",
    "        if (len(keyword_ids)<KEYWORD_MAX_SEQ_LEN):\n",
    "            keyword_ids=keyword_ids+[0]*(KEYWORD_MAX_SEQ_LEN-len(keyword_ids))\n",
    "        elif (len(keyword_ids)>KEYWORD_MAX_SEQ_LEN):\n",
    "            keyword_ids=keyword_ids[:KEYWORD_MAX_SEQ_LEN]\n",
    "\n",
    "        if self.test:\n",
    "            return torch.tensor(input_ids), torch.tensor(attention_mask), torch.tensor(keyword_ids)\n",
    "        else:\n",
    "            label=self.df.iloc[idx]['target']\n",
    "            return torch.tensor(input_ids), torch.tensor(label), torch.tensor(attention_mask), torch.tensor(keyword_ids)\n",
    "        \n",
    "train_dataset = Dataset(train_df)\n",
    "valid_dataset = Dataset(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb.shape torch.Size([32, 30])\n",
      "yb.shape torch.Size([32])\n",
      "att_mask.shape torch.Size([32, 30])\n",
      "keywords.shape torch.Size([32, 3])\n",
      "torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class ClassifierModel(torch.nn.Module):\n",
    "    def __init__(self, p_dropout=0.5):\n",
    "        super().__init__()\n",
    "        #self.model = DistilBertModel.from_pretrained('distilbert-base-multilingual-cased')\n",
    "\n",
    "        self.model = DistilBertModel.from_pretrained('distilbert-base-cased')\n",
    "        output_embedding_size=10\n",
    "        #self.freeze()\n",
    "        self.linear1 = torch.nn.Linear(768, 2)\n",
    "        #self.linear2=torch.nn.Linear(350, 2)\n",
    "        #self.maxpool=torch.nn.MaxPool1d(SEQ_LEN)\n",
    "        #self.dropout=torch.nn.Dropout(p_dropout)\n",
    "        #self.batchnorm=torch.nn.BatchNorm1d(350)\n",
    "        #self.relu=torch.nn.ReLU()\n",
    "        #self.embedding = torch.nn.Embedding(tokenizer.vocab_size, output_embedding_size)\n",
    "        \n",
    "\n",
    "    def freeze(self):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x, attention_mask=None, keywords=None):\n",
    "        #keywords_embedding = self.embedding(keywords)\n",
    "        #print(\"keywords_embedding.shape\", keywords_embedding.shape)\n",
    "        #pooled_keywords_embedding = torch.mean(keywords_embedding, dim=1)\n",
    "        #print(\"pooled_keywords_embedding.shape\", pooled_keywords_embedding.shape)\n",
    "        output = self.model(x, attention_mask=attention_mask)[\"last_hidden_state\"]\n",
    "\n",
    "        # take the first token ([CLS]) and apply classification layer on it\n",
    "        output = output[:, 0, :]\n",
    "        #print(\"output distillbert.shape\", output.shape)\n",
    "        #pooled_output = torch.mean(output, dim=1)\n",
    "        #print(\"pooled_output.shape\", pooled_output.shape)\n",
    "        #concat_output=torch.cat((pooled_output, pooled_keywords_embedding), dim=1)\n",
    "        #print(\"concat_output.shape\", output.shape)\n",
    "        output = self.linear1(output)\n",
    "        #print(\"output.shape\", output.shape)\n",
    "        #output = self.relu(output)\n",
    "        #output = self.dropout(output)\n",
    "        #output = self.batchnorm(output)\n",
    "        #output = self.linear2(output)\n",
    "        return output\n",
    "    \n",
    "my_model=ClassifierModel(p_dropout=0.5)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "for xb, yb, att_mask, keywords in train_dataloader:\n",
    "    print(\"xb.shape\", xb.shape)\n",
    "    print(\"yb.shape\", yb.shape)\n",
    "    print(\"att_mask.shape\", att_mask.shape)\n",
    "    print(\"keywords.shape\", keywords.shape)\n",
    "    print(my_model(xb, attention_mask=att_mask, keywords=keywords).shape)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.AdamW(my_model.parameters(), lr=0.0001)\n",
    "loss_fn=torch.nn.CrossEntropyLoss()\n",
    "import neptune\n",
    "\n",
    "class Learner():\n",
    "    def __init__(self, model, optimizer, loss_fn, scheduler, batch_size=32):\n",
    "        self.model=model\n",
    "        self.optimizer=optimizer\n",
    "        self.loss_fn=loss_fn\n",
    "        self.scheduler=scheduler\n",
    "        self.device=torch.device(\"cpu\")\n",
    "        if torch.cuda.is_available():\n",
    "            self.device=torch.device(\"cuda\")\n",
    "        #elif torch.backends.mps.is_available():\n",
    "        #    self.device=torch.device(\"mps\")\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.run=neptune.init_run(\n",
    "            project=\"bernd.heidemann/clickbait-classification\",\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzNjBlYzVkNi0zZTUwLTQ1ODYtODhlNC02NDUxNDg0MDdjNzUifQ==\",\n",
    "        )\n",
    "        self.batch_size=batch_size\n",
    "        self.train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def fit(self, lr=0.001, epochs=10):\n",
    "        self.run[\"parameters\"] = {\n",
    "            \"lr\": lr,\n",
    "            \"epochs\": epochs,\n",
    "            \"batch_size\": self.batch_size,\n",
    "        }\n",
    "        bar = tqdm(total=len(self.train_dataloader) * epochs, desc=\"Training\")\n",
    "        bar.set_description(\"Epoch 0/{}\".format(epochs))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()            \n",
    "            for xb, yb, att_mask, keywords in self.train_dataloader:\n",
    "                \n",
    "                xb=xb.to(self.device)\n",
    "                yb=yb.to(self.device)\n",
    "                att_mask=att_mask.to(self.device)\n",
    "                keywords=keywords.to(self.device)\n",
    "                pred=self.model(xb, attention_mask=att_mask, keywords=keywords)\n",
    "                loss=self.loss_fn(pred, yb)\n",
    "                self.run[\"train_loss\"].log(loss.item())\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                bar.update(1)\n",
    "            self.scheduler.step()\n",
    "            self.model.eval()\n",
    "            # log current state to neptune\n",
    "            metrics=self.get_accuracy()\n",
    "            self.run[\"valid_accuracy\"].log(metrics[\"accuracy\"])\n",
    "            self.run[\"valid_loss\"].log(metrics[\"loss\"])\n",
    "            \n",
    "                \n",
    "    def get_accuracy(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct=0\n",
    "            losses=[]\n",
    "            for xb, yb, att_mask, keywords in self.valid_dataloader:\n",
    "                xb=xb.to(self.device)\n",
    "                yb=yb.to(self.device)\n",
    "                att_mask=att_mask.to(self.device)\n",
    "                keywords=keywords.to(self.device)\n",
    "                pred=self.model(xb, attention_mask=att_mask, keywords=keywords)\n",
    "                loss=self.loss_fn(pred, yb)\n",
    "                losses.append(loss.item())\n",
    "                pred=torch.argmax(pred, dim=1)\n",
    "                correct+=torch.sum(pred==yb).item()\n",
    "            return {\n",
    "                \"accuracy\": correct/len(valid_dataset),\n",
    "                \"loss\": np.mean(losses)\n",
    "            }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/var/folders/yl/qjs6b9wn4zx7nh630c4my9lw0000gn/T/ipykernel_60640/4131680193.py:18: NeptuneWarning: The following monitoring options are disabled by default in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', and 'capture_hardware_metrics'. To enable them, set each parameter to 'True' when initializing the run. The monitoring will continue until you call run.stop() or the kernel stops. Also note: Your source files can only be tracked if you pass the path(s) to the 'source_code' argument. For help, see the Neptune docs: https://docs.neptune.ai/logging/source_code/\n",
      "  self.run=neptune.init_run(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/bernd.heidemann/clickbait-classification/e/CLIC-84\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5db95a380d48b3ac7530ac56293add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 5, eta_min=0.0001)\n",
    "\n",
    "my_model=ClassifierModel(p_dropout=0.5)\n",
    "learner=Learner(my_model, optimizer, loss_fn, scheduler, batch_size=128)\n",
    "learner.fit(lr=0.0001, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8266579120157583, 'loss': 0.4035767888029416}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.get_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>1680</td>\n",
       "      <td>bridge%20collapse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New: Two giant cranes holding a bridge collaps...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>5451</td>\n",
       "      <td>first%20responders</td>\n",
       "      <td>Tennessee, USA</td>\n",
       "      <td>Please pray for employees  residents and first...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>4586</td>\n",
       "      <td>emergency%20plan</td>\n",
       "      <td>In erotic world</td>\n",
       "      <td>Calgary takes another beating from summer stor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2281</th>\n",
       "      <td>7620</td>\n",
       "      <td>pandemonium</td>\n",
       "      <td>Toronto, Canada</td>\n",
       "      <td>On the Christie Hillside: Game 4 - Pandemonium...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>8548</td>\n",
       "      <td>screams</td>\n",
       "      <td>NaN</td>\n",
       "      <td>one of my fav lydia screams is in 4x11 when sh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id             keyword          location  \\\n",
       "513   1680   bridge%20collapse               NaN   \n",
       "1619  5451  first%20responders    Tennessee, USA   \n",
       "1391  4586    emergency%20plan  In erotic world    \n",
       "2281  7620         pandemonium   Toronto, Canada   \n",
       "2561  8548             screams               NaN   \n",
       "\n",
       "                                                   text  \n",
       "513   New: Two giant cranes holding a bridge collaps...  \n",
       "1619  Please pray for employees  residents and first...  \n",
       "1391  Calgary takes another beating from summer stor...  \n",
       "2281  On the Christie Hillside: Game 4 - Pandemonium...  \n",
       "2561  one of my fav lydia screams is in 4x11 when sh...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_test_path='test.csv'\n",
    "kaggle_test_path='/kaggle/input/nlp-getting-started/test.csv'\n",
    "\n",
    "df_submission_test_data=pd.read_csv(kaggle_test_path if kaggle else local_test_path)\n",
    "df_submission_test_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission_test_data=prepare_df(df_submission_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 30])\n",
      "torch.Size([32, 30])\n",
      "torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "test_dataset=Dataset(df_submission_test_data, test=True)\n",
    "test_loader=torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "for xb, att_mask, keywords in test_loader:\n",
    "    xb=xb.to(learner.device)\n",
    "    att_mask=att_mask.to(learner.device)\n",
    "    keywords=keywords.to(learner.device)\n",
    "    print(xb.shape)\n",
    "    print(att_mask.shape)\n",
    "    print(my_model(xb, attention_mask=att_mask, keywords=keywords).shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "\n",
    "for xb, att_mask, keywords in test_loader:\n",
    "    xb=xb.to(learner.device)\n",
    "    att_mask=att_mask.to(learner.device)\n",
    "    keywords=keywords.to(learner.device)\n",
    "    pred=my_model(xb, attention_mask=att_mask, keywords=keywords)\n",
    "    pred=torch.argmax(pred, dim=1)\n",
    "    predictions+=pred.tolist()\n",
    "\n",
    "df_submission_test_data['target']=predictions\n",
    "df_submission_test_data[['id', 'target']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.797"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
