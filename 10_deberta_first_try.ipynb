{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2936</th>\n",
       "      <td>location: United States | keyword: drowned | t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5294</th>\n",
       "      <td>location: United States | keyword: outbreak | ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4199</th>\n",
       "      <td>location: scandinavia | keyword: hazard | text...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>location:  | keyword: annihilated | text: Cop ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623</th>\n",
       "      <td>location: Mumbai , India | keyword: collapse |...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6489</th>\n",
       "      <td>location: Wisconsin | keyword: sunk | text: Th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2186</th>\n",
       "      <td>location: India | keyword: debris | text: MH37...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2181</th>\n",
       "      <td>location: New York | keyword: debris | text: M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3848</th>\n",
       "      <td>location:  | keyword: flames | text: I'll cry ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>location:  | keyword: body bag | text: new sum...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target\n",
       "2936  location: United States | keyword: drowned | t...       0\n",
       "5294  location: United States | keyword: outbreak | ...       1\n",
       "4199  location: scandinavia | keyword: hazard | text...       0\n",
       "238   location:  | keyword: annihilated | text: Cop ...       1\n",
       "1623  location: Mumbai , India | keyword: collapse |...       1\n",
       "6489  location: Wisconsin | keyword: sunk | text: Th...       0\n",
       "2186  location: India | keyword: debris | text: MH37...       1\n",
       "2181  location: New York | keyword: debris | text: M...       1\n",
       "3848  location:  | keyword: flames | text: I'll cry ...       0\n",
       "970   location:  | keyword: body bag | text: new sum...       0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle=False\n",
    "kaggle_path='/kaggle/input/nlp-getting-started/train.csv'\n",
    "local_path='train.csv'\n",
    "\n",
    "MODEL_CHECKPOINT = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "import os\n",
    "if kaggle:\n",
    "    os.system('pip install neptune')\n",
    "    \n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from transformers.models.auto import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "# models: https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation\n",
    "\n",
    "\n",
    "df=pd.read_csv(kaggle_path if kaggle else local_path)\n",
    "\n",
    "\n",
    "def prepare_df(df):\n",
    "    df=df.copy()\n",
    "    df.keyword=df.keyword.str.replace('%20',' ')\n",
    "    \n",
    "    # enrich text with location and text, when they are not null\n",
    "    df['text']='location: ' + df['location'].fillna('') + ' | keyword: ' +  df['keyword'].fillna('') + ' | text: ' + df['text'].fillna('')\n",
    "    # drop location and keyword\n",
    "    df=df.drop(columns=['location','keyword'])\n",
    "    # drop id\n",
    "    df=df.drop(columns=['id'])\n",
    "    return df\n",
    "\n",
    "df=prepare_df(df)\n",
    "df.sample(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# define dataset and dataloader\n",
    "\n",
    "SEQ_LEN = 50\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, test=False):\n",
    "        self.df = df\n",
    "        self.test = test\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text=self.df.iloc[idx]['text']\n",
    "\n",
    "        input_ids = self.tokenizer.encode(text, add_special_tokens=True)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        if (len(input_ids)<SEQ_LEN):\n",
    "            input_ids=input_ids+[0]*(SEQ_LEN-len(input_ids))\n",
    "            attention_mask=attention_mask+[0]*(SEQ_LEN-len(attention_mask))\n",
    "        elif (len(input_ids)>SEQ_LEN):\n",
    "            input_ids=input_ids[:SEQ_LEN]\n",
    "            attention_mask=attention_mask[:SEQ_LEN]\n",
    "\n",
    "        if self.test:\n",
    "            return torch.tensor(input_ids), torch.tensor(attention_mask)\n",
    "        else:\n",
    "            label=self.df.iloc[idx]['target']\n",
    "            return torch.tensor(input_ids), torch.tensor(label), torch.tensor(attention_mask)\n",
    "        \n",
    "train_dataset = Dataset(train_df)\n",
    "valid_dataset = Dataset(valid_df)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 50])\n",
      "torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class ClassifierModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(MODEL_CHECKPOINT, num_labels=2)\n",
    "        \n",
    "    def freeze(self):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        output = self.model(x, attention_mask=attention_mask)\n",
    "        output=output['logits']\n",
    "        return output\n",
    "    \n",
    "my_model=ClassifierModel()\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "for xb, yb, att_mask in train_dataloader:\n",
    "    print(xb.shape)\n",
    "    print(yb.shape)\n",
    "    print(att_mask.shape)\n",
    "    print(my_model(xb, attention_mask=att_mask).shape)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import neptune\n",
    "\n",
    "class Learner():\n",
    "    def __init__(self, model, train_dataloader, valid_dataloader, batch_size=32):\n",
    "        self.model=model\n",
    "        self.loss_fn=torch.nn.CrossEntropyLoss()\n",
    "        self.device=torch.device(\"cpu\")\n",
    "        if torch.cuda.is_available():\n",
    "            self.device=torch.device(\"cuda\")\n",
    "        #elif torch.backends.mps.is_available():\n",
    "        #    self.device=torch.device(\"mps\")\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.run=neptune.init_run(\n",
    "            project=\"bernd.heidemann/clickbait-classification\",\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzNjBlYzVkNi0zZTUwLTQ1ODYtODhlNC02NDUxNDg0MDdjNzUifQ==\",\n",
    "        )\n",
    "        self.batch_size=batch_size\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.valid_dataloader = valid_dataloader\n",
    "\n",
    "    def fit(self, lr=0.001, epochs=10):\n",
    "        self.run[\"parameters\"] = {\n",
    "            \"lr\": lr,\n",
    "            \"epochs\": epochs,\n",
    "            \"batch_size\": self.batch_size,\n",
    "        }\n",
    "        optimizer=torch.optim.AdamW(self.model.parameters(), lr=lr)\n",
    "        scheduler=scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "        bar = tqdm(total=len(self.train_dataloader) * epochs, desc=\"Training\")\n",
    "        bar.set_description(\"Epoch 0/{}\".format(epochs))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()            \n",
    "            for xb, yb, att_mask in self.train_dataloader:\n",
    "                \n",
    "                xb=xb.to(self.device)\n",
    "                yb=yb.to(self.device)\n",
    "                att_mask=att_mask.to(self.device)\n",
    "                pred=self.model(xb, attention_mask=att_mask)\n",
    "                loss=self.loss_fn(pred, yb)\n",
    "                self.run[\"train_loss\"].log(loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                bar.update(1)\n",
    "            scheduler.step()\n",
    "            self.model.eval()\n",
    "            # log current state to neptune\n",
    "            if self.valid_dataloader is not None:\n",
    "                metrics=self.get_accuracy()\n",
    "                self.run[\"valid_accuracy\"].log(metrics[\"accuracy\"])\n",
    "                self.run[\"valid_loss\"].log(metrics[\"loss\"])\n",
    "                bar.set_description(\"Epoch {}/{} validAccuracy: {:.2f} validLoss: {:.2f}\".format(epoch+1, epochs, metrics[\"accuracy\"], metrics[\"loss\"]))\n",
    "            \n",
    "                \n",
    "    def get_accuracy(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct=0\n",
    "            losses=[]\n",
    "            for xb, yb, att_mask in self.valid_dataloader:\n",
    "                xb=xb.to(self.device)\n",
    "                yb=yb.to(self.device)\n",
    "                att_mask=att_mask.to(self.device)\n",
    "                pred=self.model(xb, attention_mask=att_mask)\n",
    "                loss=self.loss_fn(pred, yb)\n",
    "                losses.append(loss.item())\n",
    "                pred=torch.argmax(pred, dim=1)\n",
    "                correct+=torch.sum(pred==yb).item()\n",
    "            return {\n",
    "                \"accuracy\": correct/len(valid_dataset),\n",
    "                \"loss\": np.mean(losses)\n",
    "            }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "https://app.neptune.ai/bernd.heidemann/clickbait-classification/e/CLIC-115\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c850942b59ce41a19f619c8bbd128a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/382 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "train_dataset = Dataset(train_df)\n",
    "valid_dataset = Dataset(valid_df)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "my_model=ClassifierModel()\n",
    "\n",
    "learner=Learner(my_model, train_dataloader, valid_dataloader, batch_size=128)\n",
    "learner.fit(lr=0.0001, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7806959947472094, 'loss': 0.5223116160680851}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.get_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/bernd.heidemann/clickbait-classification/e/CLIC-108\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a18b11fb42c4d608dcec8b67a0d21b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_model=ClassifierModel(p_dropout=0.5)\n",
    "full_dataset = Dataset(df, test=False)\n",
    "full_dataloader = torch.utils.data.DataLoader(full_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "learner=Learner(my_model, full_dataloader, None, batch_size=128)\n",
    "learner.fit(lr=0.0001, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2653</th>\n",
       "      <td>8870</td>\n",
       "      <td>smoke</td>\n",
       "      <td>your mom</td>\n",
       "      <td>would definitely have way more money if i didn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>6633</td>\n",
       "      <td>inundated</td>\n",
       "      <td>That London</td>\n",
       "      <td>.@38_degrees Hello. I have been inundated by p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>125</td>\n",
       "      <td>accident</td>\n",
       "      <td>Frankfurt, Germany</td>\n",
       "      <td>@DaveOshry @Soembie So if I say that I met her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>887</td>\n",
       "      <td>bioterrorism</td>\n",
       "      <td>Amsterdam NL or Greenwich USA</td>\n",
       "      <td>Is it time to hedge against catastrophic risks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>6615</td>\n",
       "      <td>inundated</td>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>WARNING: This string will be inundated with wi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       keyword                       location  \\\n",
       "2653  8870         smoke                       your mom   \n",
       "1966  6633     inundated                    That London   \n",
       "40     125      accident             Frankfurt, Germany   \n",
       "271    887  bioterrorism  Amsterdam NL or Greenwich USA   \n",
       "1962  6615     inundated                    Chicagoland   \n",
       "\n",
       "                                                   text  \n",
       "2653  would definitely have way more money if i didn...  \n",
       "1966  .@38_degrees Hello. I have been inundated by p...  \n",
       "40    @DaveOshry @Soembie So if I say that I met her...  \n",
       "271   Is it time to hedge against catastrophic risks...  \n",
       "1962  WARNING: This string will be inundated with wi...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_test_path='test.csv'\n",
    "kaggle_test_path='/kaggle/input/nlp-getting-started/test.csv'\n",
    "\n",
    "df_submission_test_data=pd.read_csv(kaggle_test_path if kaggle else local_test_path)\n",
    "df_submission_test_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>location:  | keyword: ruin | text: To respect ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2091</th>\n",
       "      <td>location:  | keyword: mayhem | text: RETWEET #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2749</th>\n",
       "      <td>location:  | keyword: suicide bomber | text: q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>location: Mo.City | keyword: blaze | text: @_A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254</th>\n",
       "      <td>location:  | keyword: oil spill | text: Refugi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "2516  location:  | keyword: ruin | text: To respect ...\n",
       "2091  location:  | keyword: mayhem | text: RETWEET #...\n",
       "2749  location:  | keyword: suicide bomber | text: q...\n",
       "289   location: Mo.City | keyword: blaze | text: @_A...\n",
       "2254  location:  | keyword: oil spill | text: Refugi..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission_test_data=prepare_df(df_submission_test_data)\n",
    "df_submission_test_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50])\n",
      "torch.Size([32, 50])\n",
      "torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "test_dataset=Dataset(df_submission_test_data, test=True)\n",
    "test_loader=torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "for xb, att_mask in test_loader:\n",
    "    xb=xb.to(learner.device)\n",
    "    att_mask=att_mask.to(learner.device)\n",
    "    print(xb.shape)\n",
    "    print(att_mask.shape)\n",
    "    print(my_model(xb, attention_mask=att_mask).shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "my_model.eval()\n",
    "\n",
    "for xb, att_mask in test_loader:\n",
    "    xb=xb.to(learner.device)\n",
    "    att_mask=att_mask.to(learner.device)\n",
    "    pred=my_model(xb, attention_mask=att_mask)\n",
    "    pred=torch.argmax(pred, dim=1)\n",
    "    predictions+=pred.tolist()\n",
    "\n",
    "\n",
    "df_submission_test_data['target']=predictions\n",
    "df_submission_test_data['id']=pd.read_csv(local_test_path)['id']\n",
    "\n",
    "\n",
    "\n",
    "df_submission_test_data[['id', 'target']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>3507</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3117</th>\n",
       "      <td>10326</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2074</th>\n",
       "      <td>6969</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>2914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1826</th>\n",
       "      <td>6176</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2176</th>\n",
       "      <td>7279</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>6080</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2805</th>\n",
       "      <td>9327</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "38      123       0\n",
       "275     901       0\n",
       "1064   3507       1\n",
       "3117  10326       0\n",
       "2074   6969       1\n",
       "883    2914       0\n",
       "1826   6176       0\n",
       "2176   7279       1\n",
       "1800   6080       0\n",
       "2805   9327       0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission_test_data[['id', 'target']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.7768"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
