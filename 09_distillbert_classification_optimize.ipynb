{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1853</th>\n",
       "      <td>location:  | keyword: crush | text: Ina Buted ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4319</th>\n",
       "      <td>location: Benicia, CA  | keyword: hellfire | t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999</th>\n",
       "      <td>location:  | keyword: twister | text: @carolin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6337</th>\n",
       "      <td>location: USA | keyword: structural failure | ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1742</th>\n",
       "      <td>location:  | keyword: collision | text: Beat:G...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5091</th>\n",
       "      <td>location: Inexpressible Island  | keyword: nuc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6823</th>\n",
       "      <td>location: New York City | keyword: trapped | t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5122</th>\n",
       "      <td>location:  | keyword: nuclear reactor | text: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2242</th>\n",
       "      <td>location: College Station, TX | keyword: delug...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5494</th>\n",
       "      <td>location: San Diego, CA | keyword: quarantine ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target\n",
       "1853  location:  | keyword: crush | text: Ina Buted ...       1\n",
       "4319  location: Benicia, CA  | keyword: hellfire | t...       0\n",
       "6999  location:  | keyword: twister | text: @carolin...       0\n",
       "6337  location: USA | keyword: structural failure | ...       1\n",
       "1742  location:  | keyword: collision | text: Beat:G...       1\n",
       "5091  location: Inexpressible Island  | keyword: nuc...       1\n",
       "6823  location: New York City | keyword: trapped | t...       0\n",
       "5122  location:  | keyword: nuclear reactor | text: ...       1\n",
       "2242  location: College Station, TX | keyword: delug...       0\n",
       "5494  location: San Diego, CA | keyword: quarantine ...       0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle=False\n",
    "kaggle_path='/kaggle/input/nlp-getting-started/train.csv'\n",
    "local_path='train.csv'\n",
    "import os\n",
    "if kaggle:\n",
    "    os.system('pip install neptune')\n",
    "    \n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "# models: https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "\n",
    "\n",
    "df=pd.read_csv(kaggle_path if kaggle else local_path)\n",
    "\n",
    "\n",
    "def prepare_df(df):\n",
    "    df=df.copy()\n",
    "    df.keyword=df.keyword.str.replace('%20',' ')\n",
    "    \n",
    "    # enrich text with location and text, when they are not null\n",
    "    df['text']='location: ' + df['location'].fillna('') + ' | keyword: ' +  df['keyword'].fillna('') + ' | text: ' + df['text'].fillna('')\n",
    "    # drop location and keyword\n",
    "    df=df.drop(columns=['location','keyword'])\n",
    "    # drop id\n",
    "    df=df.drop(columns=['id'])\n",
    "    return df\n",
    "\n",
    "df=prepare_df(df)\n",
    "df.sample(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset and dataloader\n",
    "\n",
    "SEQ_LEN = 50\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, test=False):\n",
    "        self.df = df\n",
    "        self.test = test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text=self.df.iloc[idx]['text']\n",
    "\n",
    "        input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        if (len(input_ids)<SEQ_LEN):\n",
    "            input_ids=input_ids+[0]*(SEQ_LEN-len(input_ids))\n",
    "            attention_mask=attention_mask+[0]*(SEQ_LEN-len(attention_mask))\n",
    "        elif (len(input_ids)>SEQ_LEN):\n",
    "            input_ids=input_ids[:SEQ_LEN]\n",
    "            attention_mask=attention_mask[:SEQ_LEN]\n",
    "\n",
    "        if self.test:\n",
    "            return torch.tensor(input_ids), torch.tensor(attention_mask)\n",
    "        else:\n",
    "            label=self.df.iloc[idx]['target']\n",
    "            return torch.tensor(input_ids), torch.tensor(label), torch.tensor(attention_mask)\n",
    "        \n",
    "train_dataset = Dataset(train_df)\n",
    "valid_dataset = Dataset(valid_df)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 50])\n",
      "torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class ClassifierModel(torch.nn.Module):\n",
    "    def __init__(self, p_dropout=0.5):\n",
    "        super().__init__()\n",
    "        #self.model = DistilBertModel.from_pretrained('distilbert-base-multilingual-cased')\n",
    "\n",
    "        self.model = DistilBertModel.from_pretrained('distilbert-base-cased')\n",
    "\n",
    "        #self.freeze()\n",
    "        self.linear1 = torch.nn.Linear(768, 2)\n",
    "        #self.linear2=torch.nn.Linear(350, 2)\n",
    "        #self.maxpool=torch.nn.MaxPool1d(SEQ_LEN)\n",
    "        self.dropout=torch.nn.Dropout(p_dropout)\n",
    "        #self.batchnorm=torch.nn.BatchNorm1d(350)\n",
    "        #self.relu=torch.nn.ReLU()\n",
    "        \n",
    "\n",
    "    def freeze(self):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        output = self.model(x, attention_mask=attention_mask)[\"last_hidden_state\"]\n",
    "        pooled_output = torch.mean(output, dim=1)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        output = self.linear1(pooled_output)\n",
    "        #output = self.relu(output)\n",
    "        #output = self.dropout(output)\n",
    "        #output = self.batchnorm(output)\n",
    "        #output = self.linear2(output)\n",
    "        return output\n",
    "    \n",
    "my_model=ClassifierModel(p_dropout=0.5)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "for xb, yb, att_mask in train_dataloader:\n",
    "    print(xb.shape)\n",
    "    print(yb.shape)\n",
    "    print(att_mask.shape)\n",
    "    print(my_model(xb, attention_mask=att_mask).shape)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import neptune\n",
    "\n",
    "class Learner():\n",
    "    def __init__(self, model, train_dataloader, valid_dataloader, batch_size=32):\n",
    "        self.model=model\n",
    "        self.loss_fn=torch.nn.CrossEntropyLoss()\n",
    "        self.device=torch.device(\"cpu\")\n",
    "        if torch.cuda.is_available():\n",
    "            self.device=torch.device(\"cuda\")\n",
    "        #elif torch.backends.mps.is_available():\n",
    "        #    self.device=torch.device(\"mps\")\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.run=neptune.init_run(\n",
    "            project=\"bernd.heidemann/clickbait-classification\",\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzNjBlYzVkNi0zZTUwLTQ1ODYtODhlNC02NDUxNDg0MDdjNzUifQ==\",\n",
    "        )\n",
    "        self.batch_size=batch_size\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.valid_dataloader = valid_dataloader\n",
    "\n",
    "    def fit(self, lr=0.001, epochs=10):\n",
    "        self.run[\"parameters\"] = {\n",
    "            \"lr\": lr,\n",
    "            \"epochs\": epochs,\n",
    "            \"batch_size\": self.batch_size,\n",
    "        }\n",
    "        optimizer=torch.optim.AdamW(self.model.parameters(), lr=lr)\n",
    "        scheduler=scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "        bar = tqdm(total=len(self.train_dataloader) * epochs, desc=\"Training\")\n",
    "        bar.set_description(\"Epoch 0/{}\".format(epochs))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()            \n",
    "            for xb, yb, att_mask in self.train_dataloader:\n",
    "                \n",
    "                xb=xb.to(self.device)\n",
    "                yb=yb.to(self.device)\n",
    "                att_mask=att_mask.to(self.device)\n",
    "                pred=self.model(xb, attention_mask=att_mask)\n",
    "                loss=self.loss_fn(pred, yb)\n",
    "                self.run[\"train_loss\"].log(loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                bar.update(1)\n",
    "            scheduler.step()\n",
    "            self.model.eval()\n",
    "            # log current state to neptune\n",
    "            if self.valid_dataloader is not None:\n",
    "                metrics=self.get_accuracy()\n",
    "                self.run[\"valid_accuracy\"].log(metrics[\"accuracy\"])\n",
    "                self.run[\"valid_loss\"].log(metrics[\"loss\"])\n",
    "                bar.set_description(\"Epoch {}/{} validAccuracy: {:.2f} validLoss: {:.2f}\".format(epoch+1, epochs, metrics[\"accuracy\"], metrics[\"loss\"]))\n",
    "            \n",
    "                \n",
    "    def get_accuracy(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct=0\n",
    "            losses=[]\n",
    "            for xb, yb, att_mask in self.valid_dataloader:\n",
    "                xb=xb.to(self.device)\n",
    "                yb=yb.to(self.device)\n",
    "                att_mask=att_mask.to(self.device)\n",
    "                pred=self.model(xb, attention_mask=att_mask)\n",
    "                loss=self.loss_fn(pred, yb)\n",
    "                losses.append(loss.item())\n",
    "                pred=torch.argmax(pred, dim=1)\n",
    "                correct+=torch.sum(pred==yb).item()\n",
    "            return {\n",
    "                \"accuracy\": correct/len(valid_dataset),\n",
    "                \"loss\": np.mean(losses)\n",
    "            }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shadow\\anaconda3\\envs\\mytorch\\lib\\site-packages\\neptune\\common\\warnings.py:71: NeptuneWarning: The following monitoring options are disabled by default in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', and 'capture_hardware_metrics'. To enable them, set each parameter to 'True' when initializing the run. The monitoring will continue until you call run.stop() or the kernel stops. Also note: Your source files can only be tracked if you pass the path(s) to the 'source_code' argument. For help, see the Neptune docs: https://docs.neptune.ai/logging/source_code/\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/bernd.heidemann/clickbait-classification/e/CLIC-105\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987b722d8e0c42a085bf21a5cd9fb087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/382 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "learner=Learner(my_model, train_dataloader, valid_dataloader, batch_size=128)\n",
    "learner.fit(lr=0.0001, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7806959947472094, 'loss': 0.5223116160680851}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.get_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/bernd.heidemann/clickbait-classification/e/CLIC-108\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a18b11fb42c4d608dcec8b67a0d21b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_model=ClassifierModel(p_dropout=0.5)\n",
    "full_dataset = Dataset(df, test=False)\n",
    "full_dataloader = torch.utils.data.DataLoader(full_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "learner=Learner(my_model, full_dataloader, None, batch_size=128)\n",
    "learner.fit(lr=0.0001, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2653</th>\n",
       "      <td>8870</td>\n",
       "      <td>smoke</td>\n",
       "      <td>your mom</td>\n",
       "      <td>would definitely have way more money if i didn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>6633</td>\n",
       "      <td>inundated</td>\n",
       "      <td>That London</td>\n",
       "      <td>.@38_degrees Hello. I have been inundated by p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>125</td>\n",
       "      <td>accident</td>\n",
       "      <td>Frankfurt, Germany</td>\n",
       "      <td>@DaveOshry @Soembie So if I say that I met her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>887</td>\n",
       "      <td>bioterrorism</td>\n",
       "      <td>Amsterdam NL or Greenwich USA</td>\n",
       "      <td>Is it time to hedge against catastrophic risks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>6615</td>\n",
       "      <td>inundated</td>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>WARNING: This string will be inundated with wi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       keyword                       location  \\\n",
       "2653  8870         smoke                       your mom   \n",
       "1966  6633     inundated                    That London   \n",
       "40     125      accident             Frankfurt, Germany   \n",
       "271    887  bioterrorism  Amsterdam NL or Greenwich USA   \n",
       "1962  6615     inundated                    Chicagoland   \n",
       "\n",
       "                                                   text  \n",
       "2653  would definitely have way more money if i didn...  \n",
       "1966  .@38_degrees Hello. I have been inundated by p...  \n",
       "40    @DaveOshry @Soembie So if I say that I met her...  \n",
       "271   Is it time to hedge against catastrophic risks...  \n",
       "1962  WARNING: This string will be inundated with wi...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_test_path='test.csv'\n",
    "kaggle_test_path='/kaggle/input/nlp-getting-started/test.csv'\n",
    "\n",
    "df_submission_test_data=pd.read_csv(kaggle_test_path if kaggle else local_test_path)\n",
    "df_submission_test_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>location:  | keyword: ruin | text: To respect ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2091</th>\n",
       "      <td>location:  | keyword: mayhem | text: RETWEET #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2749</th>\n",
       "      <td>location:  | keyword: suicide bomber | text: q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>location: Mo.City | keyword: blaze | text: @_A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254</th>\n",
       "      <td>location:  | keyword: oil spill | text: Refugi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "2516  location:  | keyword: ruin | text: To respect ...\n",
       "2091  location:  | keyword: mayhem | text: RETWEET #...\n",
       "2749  location:  | keyword: suicide bomber | text: q...\n",
       "289   location: Mo.City | keyword: blaze | text: @_A...\n",
       "2254  location:  | keyword: oil spill | text: Refugi..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission_test_data=prepare_df(df_submission_test_data)\n",
    "df_submission_test_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50])\n",
      "torch.Size([32, 50])\n",
      "torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "test_dataset=Dataset(df_submission_test_data, test=True)\n",
    "test_loader=torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "for xb, att_mask in test_loader:\n",
    "    xb=xb.to(learner.device)\n",
    "    att_mask=att_mask.to(learner.device)\n",
    "    print(xb.shape)\n",
    "    print(att_mask.shape)\n",
    "    print(my_model(xb, attention_mask=att_mask).shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "my_model.eval()\n",
    "\n",
    "for xb, att_mask in test_loader:\n",
    "    xb=xb.to(learner.device)\n",
    "    att_mask=att_mask.to(learner.device)\n",
    "    pred=my_model(xb, attention_mask=att_mask)\n",
    "    pred=torch.argmax(pred, dim=1)\n",
    "    predictions+=pred.tolist()\n",
    "\n",
    "\n",
    "df_submission_test_data['target']=predictions\n",
    "df_submission_test_data['id']=pd.read_csv(local_test_path)['id']\n",
    "\n",
    "\n",
    "\n",
    "df_submission_test_data[['id', 'target']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>3507</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3117</th>\n",
       "      <td>10326</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2074</th>\n",
       "      <td>6969</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>2914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1826</th>\n",
       "      <td>6176</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2176</th>\n",
       "      <td>7279</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>6080</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2805</th>\n",
       "      <td>9327</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "38      123       0\n",
       "275     901       0\n",
       "1064   3507       1\n",
       "3117  10326       0\n",
       "2074   6969       1\n",
       "883    2914       0\n",
       "1826   6176       0\n",
       "2176   7279       1\n",
       "1800   6080       0\n",
       "2805   9327       0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission_test_data[['id', 'target']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.7768"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
